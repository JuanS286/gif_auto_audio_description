{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.0.0+cu118)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.0.3)\n",
      "Collecting transformers>=3.0.0 (from bert-score)\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert-score) (1.25.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score) (3.30.3)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score) (18.1.8)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers>=3.0.0->bert-score)\n",
      "  Using cached huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers>=3.0.0->bert-score)\n",
      "  Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=3.0.0->bert-score)\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers>=3.0.0->bert-score)\n",
      "  Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2024.8.30)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=3.0.0->bert-score) (2024.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Using cached huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers, bert-score\n",
      "Successfully installed bert-score-0.3.13 huggingface-hub-0.25.2 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.20.1 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (4.66.5)\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize GCP client\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bucket details\n",
    "BUCKET_NAME = 'juanmodeltry2'\n",
    "GIF_FOLDER = 'gifs/'\n",
    "METADATA_FILE = 'metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load Metadata\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "metadata_blob = bucket.blob(METADATA_FILE)\n",
    "metadata_content = metadata_blob.download_as_text()\n",
    "metadata = json.loads(metadata_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note: If running on a system without a GPU, the code will default to CPU and still work, albeit with slower performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing Function for GIFs\n",
    "# This function preprocesses a GIF by extracting its frames, resizing, and converting them to tensors. It limits the number of frames to `max_frames` if necessary.\n",
    "def preprocess_gif(gif_blob, max_frames=16):\n",
    "    # Download the GIF as bytes from the GCP bucket\n",
    "    gif_bytes = gif_blob.download_as_bytes()\n",
    "    # Open the GIF file as an image from the in-memory byte stream\n",
    "    gif = Image.open(io.BytesIO(gif_bytes))\n",
    "    frames = []\n",
    "    \n",
    "    # Extract frames from the GIF\n",
    "    try:\n",
    "        while True:\n",
    "            frame = gif.copy().convert('RGB')  # Convert frame to RGB\n",
    "            frames.append(frame)\n",
    "            # Move to the next frame in the GIF\n",
    "            gif.seek(gif.tell() + 1)  # Move to the next frame\n",
    "    except EOFError:\n",
    "        pass\n",
    "    \n",
    "    # Limit the number of frames to max_frames by sampling if needed\n",
    "    if len(frames) > max_frames:\n",
    "        frames = random.sample(frames, max_frames)\n",
    "    \n",
    "    # Resize, normalize, and augment frames\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize frames to 224x224\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize based on ImageNet standards\n",
    "    ])\n",
    "    \n",
    "    # Apply the transformation pipeline to each frame\n",
    "    frames = [transform(frame) for frame in frames]\n",
    "    \n",
    "    # Stack frames along the temporal dimension and move to the appropriate device (CPU or GPU)\n",
    "    # Correct shape should be [batch, channels, frames, height, width]\n",
    "    stacked_frames = torch.stack(frames).permute(1, 0, 2, 3).unsqueeze(0)  # [frames, channels, height, width] -> [1, channels, frames, height, width]  # Add batch dimension to match expected ResNet3D input shape [1, channels, frames, height, width]  # [frames, channels, height, width] -> [channels, frames, height, width]\n",
    "    return stacked_frames.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load GIFs and preprocess\n",
    "def load_gifs(stage):\n",
    "    gifs = []\n",
    "    descriptions = []\n",
    "    \n",
    "    # Load the list of GIF names for the specified stage\n",
    "    if stage == 'train':\n",
    "        file_name = 'textfiles/train.txt'\n",
    "    elif stage == 'validation':\n",
    "        file_name = 'textfiles/val.txt'\n",
    "    elif stage == 'test':\n",
    "        file_name = 'textfiles/test.txt'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid stage. Must be 'train', 'validation', or 'test'.\")\n",
    "    \n",
    "    # Download the list of GIF names from GCP\n",
    "    try:\n",
    "        stage_blob = bucket.blob(file_name)\n",
    "        gif_names = stage_blob.download_as_text().splitlines()\n",
    "        print(f\"Loaded {len(gif_names)} GIF names for {stage} stage.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GIF names for {stage}: {e}\")\n",
    "        return gifs, descriptions  # Return empty lists if there is an error\n",
    "\n",
    "    # List all blobs (GIF files) in the specified bucket folder\n",
    "    blobs = storage_client.list_blobs(BUCKET_NAME, prefix=GIF_FOLDER)\n",
    "    \n",
    "    # Iterate over each blob and preprocess if it matches the required GIFs\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.gif'):\n",
    "            gif_id = blob.name.split('/')[-1]  # Extract the GIF file name (with extension)\n",
    "            if gif_id in gif_names:\n",
    "                # Find the description in the metadata based on the gif_id\n",
    "                metadata_entry = next((item for item in metadata if item['id'] == gif_id.split('.')[0]), None)\n",
    "                if metadata_entry:\n",
    "                    try:\n",
    "                        gif_tensor = preprocess_gif(blob)\n",
    "                        gifs.append(gif_tensor)\n",
    "                        descriptions.append(metadata_entry['description'])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing GIF {gif_id}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(gifs)} GIFs and {len(descriptions)} descriptions for {stage} stage.\")\n",
    "    return gifs, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 800 GIF names for train stage.\n",
      "Successfully loaded 800 GIFs and 800 descriptions for train stage.\n",
      "Loaded 100 GIF names for validation stage.\n",
      "Successfully loaded 100 GIFs and 100 descriptions for validation stage.\n",
      "Loaded 100 GIF names for test stage.\n",
      "Successfully loaded 100 GIFs and 100 descriptions for test stage.\n"
     ]
    }
   ],
   "source": [
    "# Load training, validation, and test data\n",
    "gifs_train, descriptions_train = load_gifs('train')\n",
    "gifs_val, descriptions_val = load_gifs('validation')\n",
    "gifs_test, descriptions_test = load_gifs('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# I3D Model Setup (assuming pre-trained on Kinetics)\n",
    "from torchvision.models.video import r3d_18\n",
    "# Using a pre-trained ResNet3D model from torchvision\n",
    "r3d = r3d_18(pretrained=True)\n",
    "r3d.eval()  # Set to evaluation mode for feature extraction only\n",
    "r3d = r3d.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "def extract_features(gifs):\n",
    "    features = []\n",
    "    for gif in gifs:\n",
    "        frame_features = []\n",
    "        with torch.no_grad():  # Disable gradient calculations for feature extraction\n",
    "            gif = gif.to(device)  # Move to device\n",
    "            print(f\"Input GIF shape before feature extraction: {gif.shape}\")\n",
    "            \n",
    "            # Extract features frame by frame\n",
    "            for frame_idx in range(gif.shape[2]):  # Iterate over the frames dimension\n",
    "                frame = gif[:, :, frame_idx, :, :].unsqueeze(2)  # Extract a single frame, keep dimensions [batch, channels, 1, height, width]\n",
    "                frame_feature = r3d(frame)  # Extract features from the frame using ResNet3D\n",
    "                frame_features.append(frame_feature)\n",
    "                \n",
    "            # Stack frame-wise features along the temporal dimension\n",
    "            feature = torch.stack(frame_features, dim=1)  # [batch, frames, feature_size]\n",
    "            print(f\"Feature shape after stacking frame-wise features: {feature.shape}\")\n",
    "            \n",
    "            features.append(feature.squeeze(0))  # Remove the batch dimension\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n"
     ]
    }
   ],
   "source": [
    "# Extract features from training, validation, and test GIFs\n",
    "features_train = extract_features(gifs_train)\n",
    "features_val = extract_features(gifs_val)\n",
    "features_test = extract_features(gifs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model for Caption Generation\n",
    "class GRUCaptioningModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=1, dropout=0.3):\n",
    "        super(GRUCaptioningModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        outputs, hidden = self.gru(features)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "input_dim = features_train[0].shape[-1]  # Feature size from ResNet3D (after flattening spatial dimensions)\n",
    "hidden_dim = 512  # Size of GRU hidden state\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer Setup\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Vocabulary\n",
    "all_descriptions = descriptions_train + descriptions_val + descriptions_test\n",
    "tokenizer.add_tokens(all_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the GRU model\n",
    "gru_model = GRUCaptioningModel(input_dim, hidden_dim, vocab_size, n_layers).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resize the tokenizer embedding to match new vocabulary size\n",
    "gru_model.fc = nn.Linear(hidden_dim, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizer and Loss Function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Train the GRU model\n",
    "def train_gru_model(features_train, descriptions_train, features_val, descriptions_val, gru_model, criterion, optimizer, epochs=50, batch_size=16):\n",
    "    for epoch in range(epochs):\n",
    "        gru_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training loop\n",
    "        for i in range(0, len(features_train), batch_size):\n",
    "            # Convert list of features to tensors and pad them\n",
    "            batch_features = [torch.tensor(feature).to(device) for feature in features_train[i:i+batch_size]]\n",
    "            batch_features = pad_sequence(batch_features, batch_first=True)  # Shape: (batch_size, max_seq_len, feature_size)\n",
    "            batch_descriptions = descriptions_train[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Tokenize the descriptions (target sequences)\n",
    "            tokenized_descriptions = tokenizer(batch_descriptions, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            # Forward pass through GRU\n",
    "            outputs, hidden = gru_model(batch_features)  # Get both output and hidden state from GRU\n",
    "            final_hidden_state = hidden[-1]  # Take the last layer's hidden state for each sequence\n",
    "\n",
    "            # Pass final hidden state through a linear layer to get logits\n",
    "            logits = gru_model.fc(final_hidden_state)  # Assuming gru_model has an output layer named fc\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(logits, tokenized_descriptions.view(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(features_train)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        gru_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(features_val), batch_size):\n",
    "                # Convert list of validation features to tensors and pad them\n",
    "                batch_features = [torch.tensor(feature).to(device) for feature in features_val[i:i+batch_size]]\n",
    "                batch_features = pad_sequence(batch_features, batch_first=True)  # Shape: (batch_size, max_seq_len, feature_size)\n",
    "                batch_descriptions = descriptions_val[i:i+batch_size]\n",
    "                tokenized_descriptions = tokenizer(batch_descriptions, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "                outputs, hidden = gru_model(batch_features)  # Get both output and hidden state from GRU\n",
    "                final_hidden_state = hidden[-1]  # Take the last layer's hidden state for each sequence\n",
    "                logits = gru_model.fc(final_hidden_state)\n",
    "\n",
    "                loss = criterion(logits, tokenized_descriptions.view(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(features_val)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1/3049963772.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_features = [torch.tensor(feature).to(device) for feature in features_train[i:i+batch_size]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1/3049963772.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_features = [torch.tensor(feature).to(device) for feature in features_val[i:i+batch_size]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7740\n",
      "\n",
      "Epoch [2/50], Loss: 0.4701\n",
      "Validation Loss: 0.8558\n",
      "\n",
      "Epoch [3/50], Loss: 0.4426\n",
      "Validation Loss: 0.9705\n",
      "\n",
      "Epoch [4/50], Loss: 0.4400\n",
      "Validation Loss: 0.9865\n",
      "\n",
      "Epoch [5/50], Loss: 0.4123\n",
      "Validation Loss: 0.9721\n",
      "\n",
      "Epoch [6/50], Loss: 0.3854\n",
      "Validation Loss: 0.9508\n",
      "\n",
      "Epoch [7/50], Loss: 0.3569\n",
      "Validation Loss: 0.9300\n",
      "\n",
      "Epoch [8/50], Loss: 0.3237\n",
      "Validation Loss: 0.9265\n",
      "\n",
      "Epoch [9/50], Loss: 0.2887\n",
      "Validation Loss: 0.9241\n",
      "\n",
      "Epoch [10/50], Loss: 0.2572\n",
      "Validation Loss: 0.9263\n",
      "\n",
      "Epoch [11/50], Loss: 0.2266\n",
      "Validation Loss: 0.9278\n",
      "\n",
      "Epoch [12/50], Loss: 0.1973\n",
      "Validation Loss: 0.9223\n",
      "\n",
      "Epoch [13/50], Loss: 0.1671\n",
      "Validation Loss: 0.9246\n",
      "\n",
      "Epoch [14/50], Loss: 0.1424\n",
      "Validation Loss: 0.9263\n",
      "\n",
      "Epoch [15/50], Loss: 0.1168\n",
      "Validation Loss: 0.9243\n",
      "\n",
      "Epoch [16/50], Loss: 0.0965\n",
      "Validation Loss: 0.9197\n",
      "\n",
      "Epoch [17/50], Loss: 0.0782\n",
      "Validation Loss: 0.9239\n",
      "\n",
      "Epoch [18/50], Loss: 0.0600\n",
      "Validation Loss: 0.9232\n",
      "\n",
      "Epoch [19/50], Loss: 0.0464\n",
      "Validation Loss: 0.9217\n",
      "\n",
      "Epoch [20/50], Loss: 0.0350\n",
      "Validation Loss: 0.9186\n",
      "\n",
      "Epoch [21/50], Loss: 0.0266\n",
      "Validation Loss: 0.9141\n",
      "\n",
      "Epoch [22/50], Loss: 0.0199\n",
      "Validation Loss: 0.9118\n",
      "\n",
      "Epoch [23/50], Loss: 0.0152\n",
      "Validation Loss: 0.9091\n",
      "\n",
      "Epoch [24/50], Loss: 0.0120\n",
      "Validation Loss: 0.9076\n",
      "\n",
      "Epoch [25/50], Loss: 0.0098\n",
      "Validation Loss: 0.9068\n",
      "\n",
      "Epoch [26/50], Loss: 0.0082\n",
      "Validation Loss: 0.9067\n",
      "\n",
      "Epoch [27/50], Loss: 0.0071\n",
      "Validation Loss: 0.9072\n",
      "\n",
      "Epoch [28/50], Loss: 0.0063\n",
      "Validation Loss: 0.9085\n",
      "\n",
      "Epoch [29/50], Loss: 0.0058\n",
      "Validation Loss: 0.9095\n",
      "\n",
      "Epoch [30/50], Loss: 0.0054\n",
      "Validation Loss: 0.9108\n",
      "\n",
      "Epoch [31/50], Loss: 0.0051\n",
      "Validation Loss: 0.9118\n",
      "\n",
      "Epoch [32/50], Loss: 0.0048\n",
      "Validation Loss: 0.9132\n",
      "\n",
      "Epoch [33/50], Loss: 0.0046\n",
      "Validation Loss: 0.9140\n",
      "\n",
      "Epoch [34/50], Loss: 0.0043\n",
      "Validation Loss: 0.9153\n",
      "\n",
      "Epoch [35/50], Loss: 0.0042\n",
      "Validation Loss: 0.9163\n",
      "\n",
      "Epoch [36/50], Loss: 0.0040\n",
      "Validation Loss: 0.9177\n",
      "\n",
      "Epoch [37/50], Loss: 0.0039\n",
      "Validation Loss: 0.9191\n",
      "\n",
      "Epoch [38/50], Loss: 0.0037\n",
      "Validation Loss: 0.9204\n",
      "\n",
      "Epoch [39/50], Loss: 0.0036\n",
      "Validation Loss: 0.9215\n",
      "\n",
      "Epoch [40/50], Loss: 0.0035\n",
      "Validation Loss: 0.9228\n",
      "\n",
      "Epoch [41/50], Loss: 0.0034\n",
      "Validation Loss: 0.9240\n",
      "\n",
      "Epoch [42/50], Loss: 0.0033\n",
      "Validation Loss: 0.9251\n",
      "\n",
      "Epoch [43/50], Loss: 0.0033\n",
      "Validation Loss: 0.9262\n",
      "\n",
      "Epoch [44/50], Loss: 0.0032\n",
      "Validation Loss: 0.9272\n",
      "\n",
      "Epoch [45/50], Loss: 0.0031\n",
      "Validation Loss: 0.9282\n",
      "\n",
      "Epoch [46/50], Loss: 0.0031\n",
      "Validation Loss: 0.9291\n",
      "\n",
      "Epoch [47/50], Loss: 0.0030\n",
      "Validation Loss: 0.9300\n",
      "\n",
      "Epoch [48/50], Loss: 0.0030\n",
      "Validation Loss: 0.9309\n",
      "\n",
      "Epoch [49/50], Loss: 0.0029\n",
      "Validation Loss: 0.9316\n",
      "\n",
      "Epoch [50/50], Loss: 0.0029\n",
      "Validation Loss: 0.9332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the GRU model\n",
    "train_gru_model(features_train, descriptions_train, features_val, descriptions_val, gru_model, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Loop for Test Set\n",
    "def evaluate_model(features_test, descriptions_test, gru_model, tokenizer):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')  # Ignore all warnings\n",
    "    \n",
    "    gru_model.eval()\n",
    "    total_bleu_score = 0\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    total_rouge1_score = 0\n",
    "    total_rouge2_score = 0\n",
    "    total_rougeL_score = 0\n",
    "    total_bert_f1_score = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for feature, description in zip(features_test, descriptions_test):\n",
    "            feature = feature.unsqueeze(0).to(device)  # Add batch dimension\n",
    "            \n",
    "            # Forward pass through GRU to get both output and hidden state\n",
    "            outputs, hidden = gru_model(feature)  # outputs shape: (batch_size, sequence_length, hidden_dim)\n",
    "            \n",
    "            # Take the last hidden state from the hidden tuple instead of outputs\n",
    "            # Note: hidden has the shape (n_layers, batch_size, hidden_dim)\n",
    "            last_hidden_state = hidden[-1]  # Shape: (batch_size, hidden_dim)\n",
    "            \n",
    "            # Pass the last hidden state through the fully connected layer to get logits\n",
    "            logits = gru_model.fc(last_hidden_state)  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Generate the predicted caption using argmax\n",
    "            predicted_ids = torch.argmax(logits, dim=-1).tolist()\n",
    "            predicted_caption = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # BLEU Score Calculation\n",
    "            reference = [description.split()]\n",
    "            candidate = predicted_caption.split()\n",
    "            bleu_score = sentence_bleu(reference, candidate)\n",
    "            total_bleu_score += bleu_score\n",
    "            \n",
    "            # ROUGE Score Calculation\n",
    "            rouge_scores = scorer.score(predicted_caption, description)\n",
    "            total_rouge1_score += rouge_scores['rouge1'].fmeasure\n",
    "            total_rouge2_score += rouge_scores['rouge2'].fmeasure\n",
    "            total_rougeL_score += rouge_scores['rougeL'].fmeasure\n",
    "            \n",
    "            # BERTScore Calculation (using BERTScore library)\n",
    "            from bert_score import score as bert_score\n",
    "            P, R, F1 = bert_score([predicted_caption], [description], lang=\"en\", rescale_with_baseline=True)\n",
    "            total_bert_f1_score += F1.mean().item()\n",
    "            \n",
    "            # Print each reference and generated caption for better tracking\n",
    "            print(f\"Reference: {description}\")\n",
    "            print(f\"Generated: {predicted_caption}\")\n",
    "            print(f\"BLEU Score: {bleu_score:.4f}, ROUGE-1 Score: {rouge_scores['rouge1'].fmeasure:.4f}, ROUGE-2 Score: {rouge_scores['rouge2'].fmeasure:.4f}, ROUGE-L Score: {rouge_scores['rougeL'].fmeasure:.4f}, BERT F1 Score: {F1.mean().item():.4f}\\n\")\n",
    "    \n",
    "    # Calculate the average scores\n",
    "    avg_bleu_score = total_bleu_score / len(features_test)\n",
    "    avg_rouge1_score = total_rouge1_score / len(features_test)\n",
    "    avg_rouge2_score = total_rouge2_score / len(features_test)\n",
    "    avg_rougeL_score = total_rougeL_score / len(features_test)\n",
    "    avg_bert_f1_score = total_bert_f1_score / len(features_test)\n",
    "    \n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "    print(f\"Average ROUGE-1 Score: {avg_rouge1_score:.4f}\")\n",
    "    print(f\"Average ROUGE-2 Score: {avg_rouge2_score:.4f}\")\n",
    "    print(f\"Average ROUGE-L Score: {avg_rougeL_score:.4f}\")\n",
    "    print(f\"Average BERT F1 Score: {avg_bert_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man with long blond hair is singing into a microphone\n",
      "Generated: a man turns around as he talks.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2222, ROUGE-2 Score: 0.1250, ROUGE-L Score: 0.2222, BERT F1 Score: 0.2747\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man is looking at photos and smiling\n",
      "Generated: she is point with his hands forward while talking\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1176, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1176, BERT F1 Score: 0.1709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a baby elephant tries desperately to walk but keeps slipping into the mud.\n",
      "Generated: a horses nose is zoomed in while it pokes out its tongue\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0800, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0800, BERT F1 Score: 0.2080\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a blonde girl comes out of hiding with flowers and balloons\n",
      "Generated: a boy who scares a woman to take it down\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0952, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0952, BERT F1 Score: 0.1787\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a young mother is holding a new born baby\n",
      "Generated: a black cat is leaning on a pillow\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3529, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.3529, BERT F1 Score: 0.3659\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: the woman on the treadmill can't keep up and is thrown off.\n",
      "Generated: a young man in a white shirt is hit by another man in the groin\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1429, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0714, BERT F1 Score: 0.3289\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a dog is biting his fluff leopard toy.\n",
      "Generated: a boy runs his fingers through his hair and then uses a hand gesture.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1818, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1818, BERT F1 Score: 0.1340\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man sitting in a room stares with his lips pressed together and claps.\n",
      "Generated: a man is smiling and biting his lower lip\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.4348, ROUGE-2 Score: 0.0952, ROUGE-L Score: 0.3478, BERT F1 Score: 0.3542\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: lights flashing on as a guy raises his hands and looks up\n",
      "Generated: a man in a business suit turns and looks down.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2727, ROUGE-2 Score: 0.1000, ROUGE-L Score: 0.2727, BERT F1 Score: 0.3360\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a young man is raising his hands and cheering.\n",
      "Generated: a man is rubbing his tattooed arm with his hand.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.5263, ROUGE-2 Score: 0.2353, ROUGE-L Score: 0.5263, BERT F1 Score: 0.4762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a cyclist travels through traffic in a city street\n",
      "Generated: a woman gets off a chair and places an electronic-reader on a shelf with many others.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1538, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1538, BERT F1 Score: 0.2004\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: three guys are dancing and singing in a room.\n",
      "Generated: a pony tailed girl dances next to another girl\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2222, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1111, BERT F1 Score: 0.2502\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: here is a woman with blonde hair and blue eyes.\n",
      "Generated: A girl with blue eyes and red lips is looking serious\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.5714, ROUGE-2 Score: 0.1053, ROUGE-L Score: 0.3810, BERT F1 Score: 0.3908\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a kid leans back and starts to smile\n",
      "Generated: a man is smiling and bobbing about in front of a microphone\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2000, BERT F1 Score: 0.3393\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a girl is turn off her head in a room\n",
      "Generated: a young man in a white shirt bows in a classroom\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2857, ROUGE-2 Score: 0.1053, ROUGE-L Score: 0.2857, BERT F1 Score: 0.2619\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: man with white shirt is playing basketball and jumping\n",
      "Generated: a young man is sitting while listening to music with his earplugs.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2857, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1905, BERT F1 Score: 0.2059\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: this woman is looking somewhere and smiling.\n",
      "Generated: a woman holds a small fan in front of her open mouth.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1053, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1053, BERT F1 Score: 0.3544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man wearing a green shirt is grabbing a book\n",
      "Generated: a man and woman start to kiss but the woman moves away a little.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2500, ROUGE-2 Score: 0.0909, ROUGE-L Score: 0.2500, BERT F1 Score: 0.3026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: the man in the hood is talking and pointing at his mouth.\n",
      "Generated: a woman is playing the black hair girl with sad eyes\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1739, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0870, BERT F1 Score: 0.1861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a young child is pretending to be a body builder\n",
      "Generated: a beautiful man and a cute girl with long hair are kissing\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1818, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1818, BERT F1 Score: 0.2922\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man jumps onto a mans back and gets a basketball through a hoop.\n",
      "Generated: several motorcyclists are arriving at the finish of a race\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0833, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0833, BERT F1 Score: 0.2373\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man walks in with his arms raised next to a woman sitting.\n",
      "Generated: this is a woman playing on her guitar.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1905, ROUGE-2 Score: 0.1053, ROUGE-L Score: 0.1905, BERT F1 Score: 0.3753\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: two bigger dogs smell a much smaller dog\n",
      "Generated: a man in a blue ski cap and jacket is rubbing his neck.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0952, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0952, BERT F1 Score: -0.0157\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a woman is scratching a baby goat that is wearing a blue vest\n",
      "Generated: a black cat is scratching a white dog\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3810, ROUGE-2 Score: 0.2105, ROUGE-L Score: 0.3810, BERT F1 Score: 0.3792\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a black woman looked to the side then says something.\n",
      "Generated: a man smiles and then glances away.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2353, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2353, BERT F1 Score: 0.4206\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man puts his left leg over the body of another on the floor.\n",
      "Generated: there is a guitarist who jumps on the bed and continues to play\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2222, ROUGE-2 Score: 0.0800, ROUGE-L Score: 0.2222, BERT F1 Score: 0.2539\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man with messy hair and a black jacket does not look satisfied.\n",
      "Generated: a man with glasses is eating ice cream.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2857, ROUGE-2 Score: 0.2105, ROUGE-L Score: 0.2857, BERT F1 Score: 0.3782\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man has his hand on his head and looks around.\n",
      "Generated: two girls are kissing a french kiss with each other.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0952, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0952, BERT F1 Score: 0.1315\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a woman wearing a cat costume poses and leans forward\n",
      "Generated: a man is hiding half is face and he is smiling\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1905, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1905, BERT F1 Score: 0.2855\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man in sunglasses is doing the moon walk dance.\n",
      "Generated: white cowboy hat turns into a puppet lights\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1111, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1111, BERT F1 Score: 0.1511\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a soccer team is playing a game and they are running down the field.\n",
      "Generated: a football player being pulled onto the pitch by a man in fancy dress.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2143, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1429, BERT F1 Score: 0.3587\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a woman is miming pulling a string which lifts her lip.\n",
      "Generated: two guys, dressed in red and blue striped shirts, are smiling and embracing each other.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0000, BERT F1 Score: 0.1864\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man kicks a ball toward a boy who falls backward after getting hit with the ball\n",
      "Generated: a skater is doing a spate trick off a ramp.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2222, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2222, BERT F1 Score: 0.1928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a boy is sitting with his arms wrapped around his knees\n",
      "Generated: a woman is lighting her cigarette with a lighter.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.3000, BERT F1 Score: 0.4126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man in a bear outfit scares a man coming out of a store.\n",
      "Generated: a man is dancing in a white sweater in a kitchen\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.4000, ROUGE-2 Score: 0.1739, ROUGE-L Score: 0.4000, BERT F1 Score: 0.3701\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a young woman is winking and moving her arm in circles\n",
      "Generated: a woman is taking a bite out of a pecan pie.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2727, ROUGE-2 Score: 0.1000, ROUGE-L Score: 0.2727, BERT F1 Score: 0.3113\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a woman in a leopard print dress is dancing along a sidewalk.\n",
      "Generated: a ballerina is dancing on a stage in a theater\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.5455, ROUGE-2 Score: 0.2000, ROUGE-L Score: 0.3636, BERT F1 Score: 0.4164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man holding an umbrella is twirling and dancing\n",
      "Generated: a person fell into the water and was hit by waves.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2000, BERT F1 Score: 0.3029\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man looks confused as he starts to shake his hand, then panics and shakes it some more.\n",
      "Generated: a young man is talking to someone.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2400, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2400, BERT F1 Score: 0.3670\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a guinea pig is sitting at a table and making noises\n",
      "Generated: a man smiles and puts on sunglasses as he's turning away\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1739, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1739, BERT F1 Score: 0.1574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man is caressing and kissing a woman in a swimming pool.\n",
      "Generated: two women are sitting next to each other and one laughs\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0870, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0870, BERT F1 Score: 0.2215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a boy has a scratch at his head then smiles\n",
      "Generated: the boy is playing with his lip ring.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2222, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2222, BERT F1 Score: 0.4566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man with black and gray hair is running.\n",
      "Generated: a woman riding a horse is walking toward an arena.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2105, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2105, BERT F1 Score: 0.3040\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a guy is trying to cross to the other street.\n",
      "Generated: A dog is riding on the top of a moving pickup truck\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2727, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2727, BERT F1 Score: 0.2810\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man in a bright orange jacket is talking.\n",
      "Generated: a man in a suit is doing a presentation for people round a table, one of them buries his face in the desk\n",
      "BLEU Score: 0.1090, ROUGE-1 Score: 0.3125, ROUGE-2 Score: 0.2000, ROUGE-L Score: 0.3125, BERT F1 Score: 0.3158\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man and a woman are waiting for an elevator, and the woman fights the man who is trying to grab her pocketbook.\n",
      "Generated: a woman riding a horse is walking toward an arena.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3030, ROUGE-2 Score: 0.0645, ROUGE-L Score: 0.1818, BERT F1 Score: 0.2551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man somersaults in the air and lands on the other side after being pushed.\n",
      "Generated: three boys are dancing on top of a table.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1667, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0833, BERT F1 Score: 0.3281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a woman in a red dress is standing there.\n",
      "Generated: a man uses his hands to talk on stage\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1111, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1111, BERT F1 Score: 0.2251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a little girl is dancing and falls.\n",
      "Generated: a man is holding up two fingers.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2857, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2857, BERT F1 Score: 0.2834\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a girl are sending a kiss to someone\n",
      "Generated: a young girl makes a face of disgust while brushing her hair from her face.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2609, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2609, BERT F1 Score: 0.3135\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: two girls are dancing around, with a man falling in the background.\n",
      "Generated: a man is playing an acoustic guitar in a room\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2727, ROUGE-2 Score: 0.1000, ROUGE-L Score: 0.2727, BERT F1 Score: 0.3655\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: woman with black shirt is walking and touch his hair\n",
      "Generated: two women are sitting next to each other and one laughs\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0952, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0952, BERT F1 Score: 0.1121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a woman is smiling and has glasses.\n",
      "Generated: a renaissance man is twirling a sword in a field\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2353, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2353, BERT F1 Score: 0.2378\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: there is a group of men moving their heads to the right.\n",
      "Generated: the two women are eating all together.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1053, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1053, BERT F1 Score: 0.3558\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: smoke is going in and out of a person's mouth\n",
      "Generated: the moon is setting in the sky above rooftops.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2000, BERT F1 Score: 0.0404\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man slaps his chest as he holds a microphone over his head\n",
      "Generated: a young man is sitting while listening to music with his earplugs.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2400, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2400, BERT F1 Score: 0.3554\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a soccer player does a trick shot.\n",
      "Generated: an archer aims an arrow at a target.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1333, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1333, BERT F1 Score: 0.4216\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a guy is making a few turns in a throne and then points a gun.\n",
      "Generated: a guy in concert moved across stage singing and dancing.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3200, ROUGE-2 Score: 0.0870, ROUGE-L Score: 0.3200, BERT F1 Score: 0.2556\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a cat walks on the sill of a car window, then jumps down and rolls around on the pavement.\n",
      "Generated: a cat tries to jump on top of a refrigerator and he falls down.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.4848, ROUGE-2 Score: 0.1290, ROUGE-L Score: 0.3636, BERT F1 Score: 0.4773\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a guy in a white tank top is holding an ax\n",
      "Generated: a guy scares a young girl near a car.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3000, ROUGE-2 Score: 0.1111, ROUGE-L Score: 0.3000, BERT F1 Score: 0.2973\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a white man with black glasses is moving their fake mustaches\n",
      "Generated: a famous soccer player looks up and speaks.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1053, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1053, BERT F1 Score: 0.1560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a girl is talking for some reason.\n",
      "Generated: a guy wearing a red shirt and black pants, is gliding over the ice, as he performs his routine.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1538, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1538, BERT F1 Score: 0.2174\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man is lying on his back and smoking.\n",
      "Generated: two women are signing at one another.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0000, BERT F1 Score: 0.1376\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a dog is rolling down a hill.\n",
      "Generated: a red car is driving pass a white car moving in the opposite direction\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2857, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2857, BERT F1 Score: 0.3362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man wearing a white shirt is pointing his finger.\n",
      "Generated: a man is taking his eyes down.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.4706, ROUGE-2 Score: 0.1333, ROUGE-L Score: 0.4706, BERT F1 Score: 0.3351\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man is talking into a microphone with a man standing behind him wearing a sombrero\n",
      "Generated: a man looks out the window while another man approaches from behind\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2857, ROUGE-2 Score: 0.0769, ROUGE-L Score: 0.2857, BERT F1 Score: 0.4241\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: two people are engaged in a conversation.\n",
      "Generated: two girls arguing and one goes after that.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1333, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1333, BERT F1 Score: 0.2106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a group of boys in hooded tops are gesturing\n",
      "Generated: a man throws a coaster to a woman at a bar.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1000, BERT F1 Score: 0.1831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a wrestler lifts someone over the top rope of the ring and slams him down.\n",
      "Generated: feathers are flying on a chair and a cat is jumping off\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1481, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1481, BERT F1 Score: 0.2011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man is shaking a drink mixer.\n",
      "Generated: two young men are singing on stage.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0000, BERT F1 Score: 0.4030\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a guy fakes out the defender on the basketball court causing the defender to fall down.\n",
      "Generated: a man does a back flip off a bench and lands unsuccessfully\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0714, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0714, BERT F1 Score: 0.3396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a young man being interviewed in a group of five others removes his dark glasses and smiles.\n",
      "Generated: a woman in a white dress is riding a carriage.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2222, ROUGE-2 Score: 0.0800, ROUGE-L Score: 0.2222, BERT F1 Score: 0.2983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: two people take off their black robes.\n",
      "Generated: a man in a business suit turns and looks down.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0000, BERT F1 Score: 0.2598\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: someone takes off and puts a headphone back on a girl.\n",
      "Generated: a woman wearing sunglasses sings softly into a microphone\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2000, BERT F1 Score: 0.1834\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man is drinking water under the rain.\n",
      "Generated: a person turns around and is smiling.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2667, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2667, BERT F1 Score: 0.3473\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man turns his head and is talking\n",
      "Generated: a happy man touching his hear while laughing.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3750, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.3750, BERT F1 Score: 0.2445\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man is staring at another man while in the bathroom.\n",
      "Generated: a man and woman start to kiss but the woman moves away a little.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2400, ROUGE-2 Score: 0.0870, ROUGE-L Score: 0.2400, BERT F1 Score: 0.4143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man is wrapping his arms around a distraught woman.\n",
      "Generated: a man in a blue ski cap and jacket is rubbing his neck.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.4348, ROUGE-2 Score: 0.0952, ROUGE-L Score: 0.3478, BERT F1 Score: 0.3807\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a woman with red hair wearing headphones is dancing in an elevator.\n",
      "Generated: a woman turns to her right as she sits down\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1818, ROUGE-2 Score: 0.1000, ROUGE-L Score: 0.1818, BERT F1 Score: 0.2308\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a boy rubs the chin of his friend.\n",
      "Generated: a girl is singing while shadows cross her body.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1176, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1176, BERT F1 Score: 0.4237\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: two girls are talking, they then high five\n",
      "Generated: two people in costumes are singing on stage.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2500, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2500, BERT F1 Score: 0.2602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a girl is talking to someone else indoors.\n",
      "Generated: a girl wearing a ribbon in her hair is smiling\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3333, ROUGE-2 Score: 0.1250, ROUGE-L Score: 0.3333, BERT F1 Score: 0.3669\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a person is staring at something and moving their eyes around.\n",
      "Generated: a man is looking around for something in the snow\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3810, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2857, BERT F1 Score: 0.4428\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: two people are sitting while wearing costumes.\n",
      "Generated: a man is looking around as he has a blank expression\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0000, BERT F1 Score: 0.3762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a girl in a white shirt is playing with her hair\n",
      "Generated: an old man with black hair is looking confused\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2000, BERT F1 Score: 0.4540\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man in a hard hat turns his head and smiles\n",
      "Generated: a man is break dancing and fades away into the background\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2727, ROUGE-2 Score: 0.1000, ROUGE-L Score: 0.2727, BERT F1 Score: 0.2995\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a mustached man has a questioning look on his face.\n",
      "Generated: a woman takes a drink from a bottle and offers it to another.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1739, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1739, BERT F1 Score: 0.3036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: man wearing a black shirt smiles and claps his hands\n",
      "Generated: three singers are singing and one of them is wearing sunglasses\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1905, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0952, BERT F1 Score: 0.1622\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a dancer girl is on stage dancing.\n",
      "Generated: a woman is dancing on the stage doing different types of dance\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.5263, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.5263, BERT F1 Score: 0.6143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man in front of a red car is bending backward\n",
      "Generated: a cat wearing sunglasses in a bowl yawns.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3158, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.3158, BERT F1 Score: 0.2257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: the woman is playing with her hair and blows a kiss.\n",
      "Generated: a man is wearing heart-shaped sunglasses and is dancing and singing.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2609, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1739, BERT F1 Score: 0.4083\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: two men with microphones are jumping and walking.\n",
      "Generated: a man points and winks as he sings on stage\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1111, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1111, BERT F1 Score: 0.3181\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: the man looks over before smiling and laughing.\n",
      "Generated: two robots powered by balloons collide and move around the floor\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2105, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1053, BERT F1 Score: 0.0796\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a girl singing that takes her glasses off through her hair.\n",
      "Generated: a guy wearing a red shirt and black pants, is gliding over the ice, as he performs his routine.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0667, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0667, BERT F1 Score: 0.1733\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a person in dark clothing is standing and smiling\n",
      "Generated: a man is doing a backwards flip.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2500, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2500, BERT F1 Score: 0.2818\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a handsome young man is singing with another person\n",
      "Generated: dog slept peacefully on the car seat.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0000, BERT F1 Score: 0.1404\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a large dog walks in the snow and sniffs some footprints\n",
      "Generated: a vehicle is driving through a desert spraying sand in all directions.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1739, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1739, BERT F1 Score: 0.2499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: two men are doing movements with their hands.\n",
      "Generated: a man reaches over and massages another man's neck.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0000, BERT F1 Score: 0.3603\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man is wiggling his hips and dancing on a stage.\n",
      "Generated: a person sat down singing and playing the piano.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2000, BERT F1 Score: 0.4325\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a woman moves her hips and dancing on a stage\n",
      "Generated: a young man is talking to someone.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1176, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1176, BERT F1 Score: 0.3923\n",
      "\n",
      "Average BLEU Score: 0.0011\n",
      "Average ROUGE-1 Score: 0.2196\n",
      "Average ROUGE-2 Score: 0.0343\n",
      "Average ROUGE-L Score: 0.2007\n",
      "Average BERT F1 Score: 0.2926\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the GRU model on the test set\n",
    "evaluate_model(features_test, descriptions_test, gru_model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0 (Local)",
   "language": "python",
   "name": "pytorch-2-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
