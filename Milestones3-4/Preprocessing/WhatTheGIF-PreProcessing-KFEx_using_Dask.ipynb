{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f56a893-d9e3-4105-9101-b4ff42e880c6",
   "metadata": {},
   "source": [
    "# **WhatTheGIF - PreProcessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1286cfd-fe13-4869-92d6-384e22058b65",
   "metadata": {},
   "source": [
    "The objective of this code is to create a ***PARQUET*** file with a Numpy array with all the key frames extracted from the GIFs. Those GIFs are stored in a GCP bucket. The process to read and extract the frames from the GIFs, we used Dask to paralelize that job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485fcb24-170e-4c31-ac08-0f42647ba9b2",
   "metadata": {},
   "source": [
    "## **Loading libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1b8ced-f60f-493e-b089-1b48a5be9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask import delayed, persist\n",
    "from dask.distributed import Client, wait\n",
    "import dask.bag as db\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import imageio\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import tempfile\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a8a6b1-9320-4211-8fd7-8620e393e991",
   "metadata": {},
   "source": [
    "## **Dask Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a5028-e34a-4d81-9400-381a7644ece6",
   "metadata": {},
   "source": [
    "**Dask** is a Python library for parallel and distributed computing.\n",
    "\n",
    "We deployed Dask using GCP on a Dataproc cluster, with the following steps:\n",
    "1. Create a Dataproc Cluster (whatthegif-96f2) on GCP\n",
    "\t- 1 master N workers\n",
    "\t- Running Ubuntu 22, Python 3\n",
    "\t- 1 Master HDD 200GB\n",
    "\t- 2 Nodes HDD 150GB\n",
    "\t- Machines Internal IPs:\n",
    "\t\t- [Master] 10.128.0.15\n",
    "\t\t- [Node 0] 10.128.0.14\n",
    "\t\t- [Node 1] 10.128.0.16\n",
    "2. Through SSH install dask and dependencies (master and nodes)\n",
    "\tpip install dask[complete] google-cloud-storage gcsfs\n",
    "3. Through SSH start the scheduler on master. Take note of IP:port\n",
    "\tdask scheduler\n",
    "\tMaster scheduler IP:port=tcp://10.128.0.15:8786\n",
    "4. Through SSH on nodes, start the workers\n",
    "\tdask worker 10.128.0.15:8786\n",
    "5. From local machine, connect to the scheduler\n",
    "\t35.223.219.111:8786\n",
    "\tAdd the port to the firewall rules (SSH TCP)\n",
    "6. Install on local machine\n",
    "\tpip install google-cloud-storage gcsfs dask-cloudprovider\n",
    "\tThis is important to execute a Dask job\n",
    "\tNote: Try to get the same Dask and Distributed versions on Local, Master (Scheduler) and Nodes (Workers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51ae11-19ef-47ea-8f9e-d56e160fae3b",
   "metadata": {},
   "source": [
    "### **Initialize Dask Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e382ec4-96e0-4ab3-b39b-71724b8a7e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Dask scheduler. Cluster info: <Client: 'tcp://10.128.0.15:8786' processes=2 threads=4, memory=14.90 GiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\distributed\\client.py:1613: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+----------------+----------------+----------------+\n",
      "| Package | Client         | Scheduler      | Workers        |\n",
      "+---------+----------------+----------------+----------------+\n",
      "| numpy   | 1.26.0         | 1.26.4         | 1.26.4         |\n",
      "| pandas  | 2.1.1          | 2.1.4          | 2.1.4          |\n",
      "| python  | 3.11.5.final.0 | 3.11.8.final.0 | 3.11.8.final.0 |\n",
      "| toolz   | 0.12.1         | 1.0.0          | 1.0.0          |\n",
      "+---------+----------------+----------------+----------------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    }
   ],
   "source": [
    "def create_dask_client(max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            client = Client(\"tcp://34.123.134.129:8786\", timeout='15s')\n",
    "            print(f\"Successfully connected to Dask scheduler. Cluster info: {client}\")\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            print(f\"Connection attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(15)  # Wait 15 seconds before retrying\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "# Connect to the Dask cluster\n",
    "client = create_dask_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3da76-445c-41a6-acef-16b3d17801b1",
   "metadata": {},
   "source": [
    "## **Set up Google Cloud Storage client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc84f19-e8e3-4537-a093-092fb23d8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service account credentials\n",
    "key_path = 'gcs_access.json'\n",
    "bucket_name = \"gif-bucket-1000\"\n",
    "\n",
    "# Load the service account JSON content into a variable\n",
    "with open(key_path, 'r') as f:\n",
    "    service_account_key_content = f.read()\n",
    "\n",
    "def initialize_gcs_client():\n",
    "    \"\"\"Initialize Google Cloud Storage client dynamically from credentials content.\"\"\"\n",
    "    credentials = storage.Client.from_service_account_info(json.loads(service_account_key_content))\n",
    "    return credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c0dd2-4489-4233-8c88-9225f6f77691",
   "metadata": {},
   "source": [
    "## **Defining functions needed to PreProcessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f869fb-5ce9-490a-97a4-9d1b558758a8",
   "metadata": {},
   "source": [
    "### **resize_and_normalize_frame** function\n",
    "\n",
    "Resize and normalize the frame while preserving aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a3cf6c-a99f-44a6-80f4-93cc8f7993d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_normalize_frame(frame, target_size=(256, 256)):\n",
    "    original_height, original_width = frame.shape[:2]\n",
    "    aspect_ratio = original_width / original_height\n",
    "\n",
    "    if aspect_ratio > 1:  # Wider than tall\n",
    "        new_width = target_size[0]\n",
    "        new_height = int(target_size[0] / aspect_ratio)\n",
    "    else:  # Taller than wide or square\n",
    "        new_width = int(target_size[1] * aspect_ratio)\n",
    "        new_height = target_size[1]\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "    # Calculate padding\n",
    "    delta_w = target_size[0] - new_width\n",
    "    delta_h = target_size[1] - new_height\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    # Add padding to maintain the target size\n",
    "    normalized_frame = cv2.copyMakeBorder(\n",
    "        resized_frame, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
    "    )\n",
    "    normalized_frame = normalized_frame.astype(np.float32) / 255.0\n",
    "\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9a417-8e09-47f0-a78d-c52f00e6d385",
   "metadata": {},
   "source": [
    "### **augment_frame** function\n",
    "\n",
    "Apply random data augmentation to the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5ce1ce-89c8-4d5d-b60a-06fe8d8e8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_frame(frame):\n",
    "    if np.random.rand() < 0.5:\n",
    "        frame = cv2.flip(frame, 1)  # Horizontal flip\n",
    "    if np.random.rand() < 0.5:\n",
    "        rows, cols = frame.shape[:2]\n",
    "        rotation_angle = np.random.randint(-10, 10)\n",
    "        M = cv2.getRotationMatrix2D((cols / 2, rows / 2), rotation_angle, 1)\n",
    "        frame = cv2.warpAffine(frame, M, (cols, rows))\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01bc57-06e9-45b3-b8cd-69661f99d543",
   "metadata": {},
   "source": [
    "### **extract_key_frames** function\n",
    "\n",
    "Extract key frames based on structural similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf1c582b-5e63-4443-9632-498e2d773e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_frames(frames, ssim_threshold=0.95, min_scene_change=10, max_key_frames=10):\n",
    "    key_frames = []\n",
    "    prev_frame = None\n",
    "    scene_change_counter = 0\n",
    "    scene_changes = []  # Initialize scene_changes here\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        if not isinstance(frame, np.ndarray):\n",
    "            print(f\"Frame {i}: Not a valid NumPy array.\")\n",
    "            continue\n",
    "\n",
    "        if frame.size == 0 or len(frame.shape) != 3:\n",
    "            print(\n",
    "                f\"Frame {i}: Empty or has invalid dimensions: {frame.shape if isinstance(frame, np.ndarray) else 'N/A'}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Convert to floating-point format\n",
    "        gray_frame = gray_frame.astype(np.float32) / 255.0 \n",
    "\n",
    "        if prev_frame is None:\n",
    "            key_frames.append(frame)\n",
    "            prev_frame = gray_frame\n",
    "            continue\n",
    "\n",
    "        similarity_score = ssim(prev_frame, gray_frame, data_range=1.0)\n",
    "\n",
    "        if (\n",
    "            similarity_score < ssim_threshold\n",
    "            and scene_change_counter >= min_scene_change\n",
    "            and len(key_frames) < max_key_frames\n",
    "        ):  # Check max_key_frames\n",
    "            key_frames.append(frame)\n",
    "            scene_changes.append(i)  # Add index to scene_changes\n",
    "            scene_change_counter = 0\n",
    "        else:\n",
    "            scene_change_counter = 0\n",
    "\n",
    "        prev_frame = gray_frame\n",
    "\n",
    "    # Repeat last frame if less than max_key_frames\n",
    "    while len(key_frames) < max_key_frames:\n",
    "        key_frames.append(key_frames[-1])\n",
    "\n",
    "    return key_frames, scene_changes  # Return scene_changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a79dc71-5726-499f-a21e-9e502be61cd7",
   "metadata": {},
   "source": [
    "### **process_gif** function\n",
    "\n",
    "Preprocesses a GIF from GCS and extracts 10 key frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33e0240-378f-4dd8-aa5e-2242dcf327b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gif(gif_file_name):\n",
    "    \"\"\"Preprocesses a GIF from GCS and extracts 10 key frames.\"\"\"\n",
    "    try:\n",
    "        client = initialize_gcs_client()\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        gif_blob = bucket.blob(os.path.join('gifs', gif_file_name))\n",
    "        gif_bytes = gif_blob.download_as_bytes()\n",
    "\n",
    "        with imageio.get_reader(BytesIO(gif_bytes), 'gif') as reader:\n",
    "            processed_frames = []\n",
    "            for frame in reader:\n",
    "                if frame.shape[-1] == 4:  # Convert RGBA to RGB\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)\n",
    "                resized_frame = resize_and_normalize_frame(frame)\n",
    "                augmented_frame = augment_frame(resized_frame)\n",
    "                processed_frames.append(augmented_frame)\n",
    "\n",
    "        key_frames, _ = extract_key_frames(processed_frames)\n",
    "        key_frames_np = [frame.flatten() for frame in key_frames]  # Flatten frames for storage\n",
    "        return key_frames_np\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GIF {gif_file_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b19ac6-2743-447d-9d80-cbf4b85d5e17",
   "metadata": {},
   "source": [
    "### **process_gif_safe** function\n",
    "\n",
    "Safely process a GIF, with error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0869bb20-d9d0-4aa2-889b-f8a139177ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_gif_safe(gif_file_name):\n",
    "    \"\"\"Safely process a GIF, with error handling.\"\"\"\n",
    "    try:\n",
    "        return process_gif(gif_file_name)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {gif_file_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa4fcb-95b9-4f48-8abd-a090e098857d",
   "metadata": {},
   "source": [
    "### **process_split_in_batches** function\n",
    "\n",
    "Processes a text file in smaller batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e8aa2c-0f66-4650-8889-b4e23a569003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split_in_batches(file_path, batch_size=1):\n",
    "    client = initialize_gcs_client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "    \n",
    "    try:\n",
    "        file_content = blob.download_as_string().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    lines = file_content.splitlines()\n",
    "    results = []\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i + batch_size]\n",
    "        for line in batch:\n",
    "            try:\n",
    "                gif_file, description = line.strip().split(': ')\n",
    "                key_frames = process_gif_safe(gif_file + '.gif')\n",
    "                if key_frames:\n",
    "                    results.append({\n",
    "                        \"gif_file\": gif_file, \n",
    "                        \"description\": description, \n",
    "                        \"key_frames\": key_frames\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing line {line}: {e}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb29e67-f904-4813-8c38-717554b641f6",
   "metadata": {},
   "source": [
    "## **Reading data from bucket**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55659a37-1ff3-409a-abbf-4b0e32ce4713",
   "metadata": {},
   "source": [
    "We take a sample of 2000 GIFs from the original dataset and splitted in train, test and validation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47c4fab-128a-4d8b-9be6-7c7f7a669890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify split files to process\n",
    "split_files = ['test_with_description.txt', 'val_with_description.txt']\n",
    "\n",
    "def process_gif_for_bag(line):\n",
    "    try:\n",
    "        gif_file, description = line.strip().split(': ')\n",
    "        key_frames = process_gif_safe(gif_file + '.gif')\n",
    "        if key_frames:\n",
    "            return {\n",
    "                \"gif_file\": gif_file, \n",
    "                \"description\": description, \n",
    "                \"key_frames\": key_frames\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing line {line}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_split_files(split_files):\n",
    "    results = []\n",
    "    for file_path in split_files:\n",
    "        client = initialize_gcs_client()\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file_path)\n",
    "        file_content = blob.download_as_string().decode('utf-8')\n",
    "        \n",
    "        bag = db.from_sequence(file_content.splitlines())\n",
    "        processed = bag.map(process_gif_for_bag).compute()\n",
    "        results.extend([r for r in processed if r is not None])\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df = process_split_files(split_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fc54662-ea72-44d8-ae6f-2900ed6a5a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully to GCS.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as Parquet directly to GCS\n",
    "output_path = \"gs://gif-bucket-1000/output/key_frames_data.parquet\"\n",
    "df.to_parquet(output_path, engine='pyarrow', storage_options={\"token\": key_path})\n",
    "\n",
    "print(\"Parquet file saved successfully to GCS.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
