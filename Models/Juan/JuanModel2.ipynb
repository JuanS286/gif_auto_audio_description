{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.0.0+cu118)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.0.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.45.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert-score) (1.25.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score) (3.30.3)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score) (18.1.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.25.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.20.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2024.8.30)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=3.0.0->bert-score) (2024.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.9.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score) (4.66.5)\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Using cached huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.25.2 safetensors-0.4.5 tokenizers-0.20.1 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize GCP client\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bucket details\n",
    "BUCKET_NAME = 'juanmodeltry'\n",
    "GIF_FOLDER = 'gifs/'\n",
    "METADATA_FILE = 'metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load Metadata\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "metadata_blob = bucket.blob(METADATA_FILE)\n",
    "metadata_content = metadata_blob.download_as_text()\n",
    "metadata = json.loads(metadata_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note: If running on a system without a GPU, the code will default to CPU and still work, albeit with slower performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing Function for GIFs\n",
    "# This function preprocesses a GIF by extracting its frames, resizing, and converting them to tensors. It limits the number of frames to `max_frames` if necessary.\n",
    "def preprocess_gif(gif_blob, max_frames=16):\n",
    "    # Download the GIF as bytes from the GCP bucket\n",
    "    gif_bytes = gif_blob.download_as_bytes()\n",
    "    # Open the GIF file as an image from the in-memory byte stream\n",
    "    gif = Image.open(io.BytesIO(gif_bytes))\n",
    "    frames = []\n",
    "    \n",
    "    # Extract frames from the GIF\n",
    "    try:\n",
    "        while True:\n",
    "            frame = gif.copy().convert('RGB')  # Convert frame to RGB\n",
    "            frames.append(frame)\n",
    "            # Move to the next frame in the GIF\n",
    "            gif.seek(gif.tell() + 1)  # Move to the next frame\n",
    "    except EOFError:\n",
    "        pass\n",
    "    \n",
    "    # Limit the number of frames to max_frames by sampling if needed\n",
    "    if len(frames) > max_frames:\n",
    "        frames = random.sample(frames, max_frames)\n",
    "    \n",
    "    # Resize, normalize, and augment frames\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize frames to 224x224\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize based on ImageNet standards\n",
    "    ])\n",
    "    \n",
    "    # Apply the transformation pipeline to each frame\n",
    "    frames = [transform(frame) for frame in frames]\n",
    "    \n",
    "    # Stack frames along the temporal dimension and move to the appropriate device (CPU or GPU)\n",
    "    # Correct shape should be [batch, channels, frames, height, width]\n",
    "    stacked_frames = torch.stack(frames).permute(1, 0, 2, 3).unsqueeze(0)  # [frames, channels, height, width] -> [1, channels, frames, height, width]  # Add batch dimension to match expected ResNet3D input shape [1, channels, frames, height, width]  # [frames, channels, height, width] -> [channels, frames, height, width]\n",
    "    return stacked_frames.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load GIFs and preprocess\n",
    "def load_gifs(stage):\n",
    "    gifs = []\n",
    "    descriptions = []\n",
    "    \n",
    "    # Load the list of GIF names for the specified stage\n",
    "    if stage == 'train':\n",
    "        file_name = 'textfiles/train.txt'\n",
    "    elif stage == 'validation':\n",
    "        file_name = 'textfiles/val.txt'\n",
    "    elif stage == 'test':\n",
    "        file_name = 'textfiles/test.txt'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid stage. Must be 'train', 'validation', or 'test'.\")\n",
    "    \n",
    "    # Download the list of GIF names from GCP\n",
    "    try:\n",
    "        stage_blob = bucket.blob(file_name)\n",
    "        gif_names = stage_blob.download_as_text().splitlines()\n",
    "        print(f\"Loaded {len(gif_names)} GIF names for {stage} stage.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GIF names for {stage}: {e}\")\n",
    "        return gifs, descriptions  # Return empty lists if there is an error\n",
    "\n",
    "    # List all blobs (GIF files) in the specified bucket folder\n",
    "    blobs = storage_client.list_blobs(BUCKET_NAME, prefix=GIF_FOLDER)\n",
    "    \n",
    "    # Iterate over each blob and preprocess if it matches the required GIFs\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.gif'):\n",
    "            gif_id = blob.name.split('/')[-1]  # Extract the GIF file name (with extension)\n",
    "            if gif_id in gif_names:\n",
    "                # Find the description in the metadata based on the gif_id\n",
    "                metadata_entry = next((item for item in metadata if item['id'] == gif_id.split('.')[0]), None)\n",
    "                if metadata_entry:\n",
    "                    try:\n",
    "                        gif_tensor = preprocess_gif(blob)\n",
    "                        gifs.append(gif_tensor)\n",
    "                        descriptions.append(metadata_entry['description'])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing GIF {gif_id}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(gifs)} GIFs and {len(descriptions)} descriptions for {stage} stage.\")\n",
    "    return gifs, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 70 GIF names for train stage.\n",
      "Successfully loaded 70 GIFs and 70 descriptions for train stage.\n",
      "Loaded 10 GIF names for validation stage.\n",
      "Successfully loaded 10 GIFs and 10 descriptions for validation stage.\n",
      "Loaded 10 GIF names for test stage.\n",
      "Successfully loaded 10 GIFs and 10 descriptions for test stage.\n"
     ]
    }
   ],
   "source": [
    "# Load training, validation, and test data\n",
    "gifs_train, descriptions_train = load_gifs('train')\n",
    "gifs_val, descriptions_val = load_gifs('validation')\n",
    "gifs_test, descriptions_test = load_gifs('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# I3D Model Setup (assuming pre-trained on Kinetics)\n",
    "from torchvision.models.video import r3d_18\n",
    "# Using a pre-trained ResNet3D model from torchvision\n",
    "r3d = r3d_18(pretrained=True)\n",
    "r3d.eval()  # Set to evaluation mode for feature extraction only\n",
    "r3d = r3d.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "def extract_features(gifs):\n",
    "    features = []\n",
    "    for gif in gifs:\n",
    "        frame_features = []\n",
    "        with torch.no_grad():  # Disable gradient calculations for feature extraction\n",
    "            gif = gif.to(device)  # Move to device\n",
    "            print(f\"Input GIF shape before feature extraction: {gif.shape}\")\n",
    "            \n",
    "            # Extract features frame by frame\n",
    "            for frame_idx in range(gif.shape[2]):  # Iterate over the frames dimension\n",
    "                frame = gif[:, :, frame_idx, :, :].unsqueeze(2)  # Extract a single frame, keep dimensions [batch, channels, 1, height, width]\n",
    "                frame_feature = r3d(frame)  # Extract features from the frame using ResNet3D\n",
    "                frame_features.append(frame_feature)\n",
    "                \n",
    "            # Stack frame-wise features along the temporal dimension\n",
    "            feature = torch.stack(frame_features, dim=1)  # [batch, frames, feature_size]\n",
    "            print(f\"Feature shape after stacking frame-wise features: {feature.shape}\")\n",
    "            \n",
    "            features.append(feature.squeeze(0))  # Remove the batch dimension\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 1, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 1, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n",
      "Input GIF shape before feature extraction: torch.Size([1, 3, 16, 224, 224])\n",
      "Feature shape after stacking frame-wise features: torch.Size([1, 16, 400])\n"
     ]
    }
   ],
   "source": [
    "# Extract features from training, validation, and test GIFs\n",
    "features_train = extract_features(gifs_train)\n",
    "features_val = extract_features(gifs_val)\n",
    "features_test = extract_features(gifs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model for Caption Generation\n",
    "class GRUCaptioningModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=1, dropout=0.3):\n",
    "        super(GRUCaptioningModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        outputs, hidden = self.gru(features)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "input_dim = features_train[0].shape[-1]  # Feature size from ResNet3D (after flattening spatial dimensions)\n",
    "hidden_dim = 512  # Size of GRU hidden state\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer Setup\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Vocabulary\n",
    "all_descriptions = descriptions_train + descriptions_val + descriptions_test\n",
    "tokenizer.add_tokens(all_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the GRU model\n",
    "gru_model = GRUCaptioningModel(input_dim, hidden_dim, vocab_size, n_layers).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resize the tokenizer embedding to match new vocabulary size\n",
    "gru_model.fc = nn.Linear(hidden_dim, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizer and Loss Function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Train the GRU model\n",
    "def train_gru_model(features_train, descriptions_train, features_val, descriptions_val, gru_model, criterion, optimizer, epochs=50, batch_size=16):\n",
    "    for epoch in range(epochs):\n",
    "        gru_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training loop\n",
    "        for i in range(0, len(features_train), batch_size):\n",
    "            # Convert list of features to tensors and pad them\n",
    "            batch_features = [torch.tensor(feature).to(device) for feature in features_train[i:i+batch_size]]\n",
    "            batch_features = pad_sequence(batch_features, batch_first=True)  # Shape: (batch_size, max_seq_len, feature_size)\n",
    "            batch_descriptions = descriptions_train[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Tokenize the descriptions (target sequences)\n",
    "            tokenized_descriptions = tokenizer(batch_descriptions, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "            # Forward pass through GRU\n",
    "            outputs, hidden = gru_model(batch_features)  # Get both output and hidden state from GRU\n",
    "            final_hidden_state = hidden[-1]  # Take the last layer's hidden state for each sequence\n",
    "\n",
    "            # Pass final hidden state through a linear layer to get logits\n",
    "            logits = gru_model.fc(final_hidden_state)  # Assuming gru_model has an output layer named fc\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(logits, tokenized_descriptions.view(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(features_train)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        gru_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(features_val), batch_size):\n",
    "                # Convert list of validation features to tensors and pad them\n",
    "                batch_features = [torch.tensor(feature).to(device) for feature in features_val[i:i+batch_size]]\n",
    "                batch_features = pad_sequence(batch_features, batch_first=True)  # Shape: (batch_size, max_seq_len, feature_size)\n",
    "                batch_descriptions = descriptions_val[i:i+batch_size]\n",
    "                tokenized_descriptions = tokenizer(batch_descriptions, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "                outputs, hidden = gru_model(batch_features)  # Get both output and hidden state from GRU\n",
    "                final_hidden_state = hidden[-1]  # Take the last layer's hidden state for each sequence\n",
    "                logits = gru_model.fc(final_hidden_state)\n",
    "\n",
    "                loss = criterion(logits, tokenized_descriptions.view(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(features_val)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1/3049963772.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_features = [torch.tensor(feature).to(device) for feature in features_train[i:i+batch_size]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.7719\n",
      "Validation Loss: 1.0691\n",
      "\n",
      "Epoch [2/50], Loss: 0.5394\n",
      "Validation Loss: 1.0850\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1/3049963772.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_features = [torch.tensor(feature).to(device) for feature in features_val[i:i+batch_size]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.3517\n",
      "Validation Loss: 1.1707\n",
      "\n",
      "Epoch [4/50], Loss: 0.2300\n",
      "Validation Loss: 1.2513\n",
      "\n",
      "Epoch [5/50], Loss: 0.1633\n",
      "Validation Loss: 1.2833\n",
      "\n",
      "Epoch [6/50], Loss: 0.1190\n",
      "Validation Loss: 1.3200\n",
      "\n",
      "Epoch [7/50], Loss: 0.0873\n",
      "Validation Loss: 1.3211\n",
      "\n",
      "Epoch [8/50], Loss: 0.0629\n",
      "Validation Loss: 1.3149\n",
      "\n",
      "Epoch [9/50], Loss: 0.0443\n",
      "Validation Loss: 1.3090\n",
      "\n",
      "Epoch [10/50], Loss: 0.0310\n",
      "Validation Loss: 1.2977\n",
      "\n",
      "Epoch [11/50], Loss: 0.0225\n",
      "Validation Loss: 1.2853\n",
      "\n",
      "Epoch [12/50], Loss: 0.0170\n",
      "Validation Loss: 1.2736\n",
      "\n",
      "Epoch [13/50], Loss: 0.0135\n",
      "Validation Loss: 1.2608\n",
      "\n",
      "Epoch [14/50], Loss: 0.0113\n",
      "Validation Loss: 1.2486\n",
      "\n",
      "Epoch [15/50], Loss: 0.0098\n",
      "Validation Loss: 1.2368\n",
      "\n",
      "Epoch [16/50], Loss: 0.0088\n",
      "Validation Loss: 1.2246\n",
      "\n",
      "Epoch [17/50], Loss: 0.0081\n",
      "Validation Loss: 1.2137\n",
      "\n",
      "Epoch [18/50], Loss: 0.0075\n",
      "Validation Loss: 1.2044\n",
      "\n",
      "Epoch [19/50], Loss: 0.0070\n",
      "Validation Loss: 1.1963\n",
      "\n",
      "Epoch [20/50], Loss: 0.0066\n",
      "Validation Loss: 1.1894\n",
      "\n",
      "Epoch [21/50], Loss: 0.0062\n",
      "Validation Loss: 1.1836\n",
      "\n",
      "Epoch [22/50], Loss: 0.0059\n",
      "Validation Loss: 1.1786\n",
      "\n",
      "Epoch [23/50], Loss: 0.0056\n",
      "Validation Loss: 1.1746\n",
      "\n",
      "Epoch [24/50], Loss: 0.0053\n",
      "Validation Loss: 1.1712\n",
      "\n",
      "Epoch [25/50], Loss: 0.0050\n",
      "Validation Loss: 1.1684\n",
      "\n",
      "Epoch [26/50], Loss: 0.0048\n",
      "Validation Loss: 1.1659\n",
      "\n",
      "Epoch [27/50], Loss: 0.0046\n",
      "Validation Loss: 1.1638\n",
      "\n",
      "Epoch [28/50], Loss: 0.0044\n",
      "Validation Loss: 1.1618\n",
      "\n",
      "Epoch [29/50], Loss: 0.0042\n",
      "Validation Loss: 1.1601\n",
      "\n",
      "Epoch [30/50], Loss: 0.0041\n",
      "Validation Loss: 1.1585\n",
      "\n",
      "Epoch [31/50], Loss: 0.0039\n",
      "Validation Loss: 1.1571\n",
      "\n",
      "Epoch [32/50], Loss: 0.0038\n",
      "Validation Loss: 1.1559\n",
      "\n",
      "Epoch [33/50], Loss: 0.0036\n",
      "Validation Loss: 1.1548\n",
      "\n",
      "Epoch [34/50], Loss: 0.0035\n",
      "Validation Loss: 1.1538\n",
      "\n",
      "Epoch [35/50], Loss: 0.0034\n",
      "Validation Loss: 1.1529\n",
      "\n",
      "Epoch [36/50], Loss: 0.0033\n",
      "Validation Loss: 1.1521\n",
      "\n",
      "Epoch [37/50], Loss: 0.0032\n",
      "Validation Loss: 1.1514\n",
      "\n",
      "Epoch [38/50], Loss: 0.0031\n",
      "Validation Loss: 1.1508\n",
      "\n",
      "Epoch [39/50], Loss: 0.0031\n",
      "Validation Loss: 1.1503\n",
      "\n",
      "Epoch [40/50], Loss: 0.0030\n",
      "Validation Loss: 1.1499\n",
      "\n",
      "Epoch [41/50], Loss: 0.0029\n",
      "Validation Loss: 1.1496\n",
      "\n",
      "Epoch [42/50], Loss: 0.0028\n",
      "Validation Loss: 1.1493\n",
      "\n",
      "Epoch [43/50], Loss: 0.0028\n",
      "Validation Loss: 1.1491\n",
      "\n",
      "Epoch [44/50], Loss: 0.0027\n",
      "Validation Loss: 1.1489\n",
      "\n",
      "Epoch [45/50], Loss: 0.0026\n",
      "Validation Loss: 1.1488\n",
      "\n",
      "Epoch [46/50], Loss: 0.0026\n",
      "Validation Loss: 1.1488\n",
      "\n",
      "Epoch [47/50], Loss: 0.0025\n",
      "Validation Loss: 1.1488\n",
      "\n",
      "Epoch [48/50], Loss: 0.0025\n",
      "Validation Loss: 1.1488\n",
      "\n",
      "Epoch [49/50], Loss: 0.0024\n",
      "Validation Loss: 1.1488\n",
      "\n",
      "Epoch [50/50], Loss: 0.0024\n",
      "Validation Loss: 1.1489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the GRU model\n",
    "train_gru_model(features_train, descriptions_train, features_val, descriptions_val, gru_model, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Loop for Test Set\n",
    "def evaluate_model(features_test, descriptions_test, gru_model, tokenizer):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')  # Ignore all warnings\n",
    "    \n",
    "    gru_model.eval()\n",
    "    total_bleu_score = 0\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    total_rouge1_score = 0\n",
    "    total_rouge2_score = 0\n",
    "    total_rougeL_score = 0\n",
    "    total_bert_f1_score = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for feature, description in zip(features_test, descriptions_test):\n",
    "            feature = feature.unsqueeze(0).to(device)  # Add batch dimension\n",
    "            \n",
    "            # Forward pass through GRU to get both output and hidden state\n",
    "            outputs, hidden = gru_model(feature)  # outputs shape: (batch_size, sequence_length, hidden_dim)\n",
    "            \n",
    "            # Take the last hidden state from the hidden tuple instead of outputs\n",
    "            # Note: hidden has the shape (n_layers, batch_size, hidden_dim)\n",
    "            last_hidden_state = hidden[-1]  # Shape: (batch_size, hidden_dim)\n",
    "            \n",
    "            # Pass the last hidden state through the fully connected layer to get logits\n",
    "            logits = gru_model.fc(last_hidden_state)  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Generate the predicted caption using argmax\n",
    "            predicted_ids = torch.argmax(logits, dim=-1).tolist()\n",
    "            predicted_caption = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # BLEU Score Calculation\n",
    "            reference = [description.split()]\n",
    "            candidate = predicted_caption.split()\n",
    "            bleu_score = sentence_bleu(reference, candidate)\n",
    "            total_bleu_score += bleu_score\n",
    "            \n",
    "            # ROUGE Score Calculation\n",
    "            rouge_scores = scorer.score(predicted_caption, description)\n",
    "            total_rouge1_score += rouge_scores['rouge1'].fmeasure\n",
    "            total_rouge2_score += rouge_scores['rouge2'].fmeasure\n",
    "            total_rougeL_score += rouge_scores['rougeL'].fmeasure\n",
    "            \n",
    "            # BERTScore Calculation (using BERTScore library)\n",
    "            from bert_score import score as bert_score\n",
    "            P, R, F1 = bert_score([predicted_caption], [description], lang=\"en\", rescale_with_baseline=True)\n",
    "            total_bert_f1_score += F1.mean().item()\n",
    "            \n",
    "            # Print each reference and generated caption for better tracking\n",
    "            print(f\"Reference: {description}\")\n",
    "            print(f\"Generated: {predicted_caption}\")\n",
    "            print(f\"BLEU Score: {bleu_score:.4f}, ROUGE-1 Score: {rouge_scores['rouge1'].fmeasure:.4f}, ROUGE-2 Score: {rouge_scores['rouge2'].fmeasure:.4f}, ROUGE-L Score: {rouge_scores['rougeL'].fmeasure:.4f}, BERT F1 Score: {F1.mean().item():.4f}\\n\")\n",
    "    \n",
    "    # Calculate the average scores\n",
    "    avg_bleu_score = total_bleu_score / len(features_test)\n",
    "    avg_rouge1_score = total_rouge1_score / len(features_test)\n",
    "    avg_rouge2_score = total_rouge2_score / len(features_test)\n",
    "    avg_rougeL_score = total_rougeL_score / len(features_test)\n",
    "    avg_bert_f1_score = total_bert_f1_score / len(features_test)\n",
    "    \n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "    print(f\"Average ROUGE-1 Score: {avg_rouge1_score:.4f}\")\n",
    "    print(f\"Average ROUGE-2 Score: {avg_rouge2_score:.4f}\")\n",
    "    print(f\"Average ROUGE-L Score: {avg_rougeL_score:.4f}\")\n",
    "    print(f\"Average BERT F1 Score: {avg_bert_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: this is an animal walking through a trail in the snow on a leash.\n",
      "Generated: a rose is opening against a blue sky.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2727, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1818, BERT F1 Score: 0.2415\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: seagulls fly around the water's edge at the beach.\n",
      "Generated: a car drives too fast and almost kills a woman.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0000, BERT F1 Score: 0.1485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a ball flies through the air and a cat hits it into the mouth of a dog.\n",
      "Generated: a large dog lets small dog out of a cage and they walk off together.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3125, ROUGE-2 Score: 0.0667, ROUGE-L Score: 0.1875, BERT F1 Score: 0.2819\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: this is a man talking and then pets a panther.\n",
      "Generated: a woman is sitting in a truck and putting her feet up.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3636, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2727, BERT F1 Score: 0.3435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man activates a spinning wheel wrapped around his head.\n",
      "Generated: a woman looks at another person and talks with them.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1000, BERT F1 Score: 0.2923\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a man with a beard and mustache is speaking.\n",
      "Generated: several young women are preforming dance moves and singing\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1111, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1111, BERT F1 Score: 0.2356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: three people are singing, and one pats the other on the behind.\n",
      "Generated: a man wearing a shirt with a stripe is using a martial arts weapon\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.0000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.0000, BERT F1 Score: 0.2227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a young boy is eating and laughing.\n",
      "Generated: a man in a tank top and underwear is dancing towards a woman.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.3000, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2000, BERT F1 Score: 0.4720\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a person is playing with a dog.\n",
      "Generated: a woman is sitting in a car and moving the sleeve of her blue jumper to her mouth.\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.2400, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.2400, BERT F1 Score: 0.3016\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: a kitten leans its head back and bounces against his owner.\n",
      "Generated: a man lifts the cover off of a plate and the woman picks up a purple bracelet\n",
      "BLEU Score: 0.0000, ROUGE-1 Score: 0.1429, ROUGE-2 Score: 0.0000, ROUGE-L Score: 0.1429, BERT F1 Score: 0.1860\n",
      "\n",
      "Average BLEU Score: 0.0000\n",
      "Average ROUGE-1 Score: 0.1843\n",
      "Average ROUGE-2 Score: 0.0067\n",
      "Average ROUGE-L Score: 0.1436\n",
      "Average BERT F1 Score: 0.2726\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the GRU model on the test set\n",
    "evaluate_model(features_test, descriptions_test, gru_model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0 (Local)",
   "language": "python",
   "name": "pytorch-2-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
